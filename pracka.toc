\babel@toc {english}{}
\contentsline {chapter}{Introduction}{5}%
\contentsline {chapter}{\numberline {1}Background}{7}%
\contentsline {section}{\numberline {1.1}Time series and notation}{7}%
\contentsline {section}{\numberline {1.2}Transformer model architecture}{7}%
\contentsline {subsection}{\numberline {1.2.1}Overview}{7}%
\contentsline {section}{\numberline {1.3}Attention}{8}%
\contentsline {subsection}{\numberline {1.3.1}Scaled Dot-Product Attention}{8}%
\contentsline {subsection}{\numberline {1.3.2}Convolutional Attention}{9}%
\contentsline {subsection}{\numberline {1.3.3}Multi-headed attention}{9}%
\contentsline {section}{\numberline {1.4}Training}{10}%
\contentsline {subsection}{\numberline {1.4.1}Additional input}{10}%
\contentsline {subsection}{\numberline {1.4.2}Scaling and weighted sampling}{10}%
\contentsline {section}{\numberline {1.5}Metrics}{10}%
\contentsline {subsection}{\numberline {1.5.1}Quantile loss}{10}%
\contentsline {subsection}{\numberline {1.5.2}Risk metric}{11}%
\contentsline {chapter}{\numberline {2}Approximating a continuous distribution with the Transformer}{13}%
\contentsline {section}{\numberline {2.1}Serial Predictor}{13}%
\contentsline {subsection}{\numberline {2.1.1}Serialization}{14}%
\contentsline {subsection}{\numberline {2.1.2}Architecture's summary}{14}%
\contentsline {chapter}{\numberline {3}Experiments}{15}%
\contentsline {section}{\numberline {3.1}Datasets}{15}%
\contentsline {section}{\numberline {3.2}Differences from previous works}{15}%
\contentsline {section}{\numberline {3.3}Experiments setup}{16}%
\contentsline {section}{\numberline {3.4}Results}{16}%
\contentsline {chapter}{\numberline {4}Conclusion}{19}%
\contentsline {section}{\numberline {4.1}Summary}{19}%
\contentsline {section}{\numberline {4.2}Further work}{19}%
\contentsline {chapter}{Bibliography}{21}%
