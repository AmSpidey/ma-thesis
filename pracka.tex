%
% Niniejszy plik stanowi przykład formatowania pracy magisterskiej na
% Wydziale MIM UW.  Szkielet użytych poleceń można wykorzystywać do
% woli, np. formatujac wlasna prace.
%
% Zawartosc merytoryczna stanowi oryginalnosiagniecie
% naukowosciowe Marcina Wolinskiego.  Wszelkie prawa zastrzeżone.
%
% Copyright (c) 2001 by Marcin Woliński <M.Wolinski@gust.org.pl>
% Poprawki spowodowane zmianami przepisów - Marcin Szczuka, 1.10.2004
% Poprawki spowodowane zmianami przepisow i ujednolicenie 
% - Seweryn Karłowicz, 05.05.2006
% Dodanie wielu autorów i tłumaczenia na angielski - Kuba Pochrybniak, 29.11.2016

% dodaj opcję [licencjacka] dla pracy licencjackiej
% dodaj opcję [en] dla wersji angielskiej (mogą być obie: [licencjacka,en])

\documentclass[en]{pracamgr}

\usepackage{amsmath}


%% Imports the package natbib
\usepackage[square,numbers]{natbib}

%% Sets the bibliography style
\bibliographystyle{abbrvnat}

% Dane magistranta:
\autor{Marta Nowakowska}{385914}

% Dane magistrantów:
%\autor{Autor Zerowy}{342007}
%\autori{Autor Pierwszy}{342013}
%\autorii{Drugi Autor-Z-Rzędu}{231023}
%\autoriii{Trzeci z Autorów}{777321}
%\autoriv{Autor nr Cztery}{432145}
%\autorv{Autor nr Pięć}{342011}

\title{Time series prediction using self-attention models}
\titlepl{Prognozowanie szeregów czasowych przy użyciu modeli z uwagą}

%\tytulang{An implementation of a difference blabalizer based on the theory of $\sigma$ -- $\rho$ phetors}

%kierunek: 
% - matematyka, informacyka, ...
% - Mathematics, Computer Science, ...
\kierunek{Computer Science}

% informatyka - nie okreslamy zakresu (opcja zakomentowana)
% matematyka - zakres moze pozostac nieokreslony,
% a jesli ma byc okreslony dla pracy mgr,
% to przyjmuje jedna z wartosci:
% {metod matematycznych w finansach}
% {metod matematycznych w ubezpieczeniach}
% {matematyki stosowanej}
% {nauczania matematyki}
% Dla pracy licencjackiej mamy natomiast
% mozliwosc wpisania takiej wartosci zakresu:
% {Jednoczesnych Studiow Ekonomiczno--Matematycznych}

% \zakres{Tu wpisac, jesli trzeba, jedna z opcji podanych wyzej}

% Praca wykonana pod kierunkiem:
% (podać tytuł/stopień imię i nazwisko opiekuna
% Instytut
% ew. Wydział ew. Uczelnia (jeżeli nie MIM UW))
\opiekun{dr hab. Piotr Miłoś\\
  Institute of Mathematics, Polish Academy of Sciences\\
  }

\copiekun{dr hab. Marek Cygan, prof. UW\\
	University of Warsaw\\
}

%\coopiekun{dr Piotr Miłoś\\
%	Institute of Mathematics, Polish Academy of Sciences\\
%}

% miesiąc i~rok:
\date{August 2022}

%Podać dziedzinę wg klasyfikacji Socrates-Erasmus:
\dziedzina{ 
%11.0 Matematyka, Informatyka:\\ 
%11.1 Matematyka\\ 
%11.2 Statystyka\\ 
%11.3 Informatyka\\ 
11.4 Sztuczna inteligencja\\ 
%11.5 Nauki aktuarialne\\
%11.9 Inne nauki matematyczne i informatyczne
}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{D. Software\\
  D.127. Blabalgorithms\\
  D.127.6. Numerical blabalysis}

% Słowa kluczowe:
\keywords{time series, machine learning, self-attention model}

% Tu jest dobre miejsce na Twoje własne makra i~środowiska:
\newtheorem{defi}{Definition}[section]

% koniec definicji

\begin{document}
\maketitle

%tu idzie streszczenie na strone poczatkowa
\begin{abstract}
  TODO abstract
\end{abstract}

\tableofcontents
%\listoffigures
%\listoftables

\chapter{Introduction}
%\addcontentsline{toc}{chapter}{Time series and their importance}
\section{Time series and their importance}\label{r:tsi}

% retail forecasting (supply, demand)
% analiza danych giełdowych
% Time series forecasting occurs when you make scientific predictions based on historical time stamped data. It
% Time series forecasting is considered one of the most applied data science techniques that are used in different industries such as finance, supply chain management, production, and inventory planning. 
% Stock prices forecasting, weather forecasting, business planning, resource allocation are only a few of the many possible applications for time series forecasting.

Time series is a sequence of data points measured across time with a fixed frequency.
It is one of the most widespread type of data, making its forecasting incredibly useful for vast industries and research areas. Among others they are being used for stock market analysis, weather prediction and resource allocation planning.

% w ehancing: "Although
%still widely used, traditional time series forecasting models, such as State Space Models (SSMs) [2]
%and Autoregressive (AR) models, are designed to fit each time series independently. Besides, they
%also require practitioners’ expertise in manually selecting trend, seasonality and other components.
%To sum up, these two major weaknesses have greatly hindered their applications in the modern
%large-scale time series forecasting tasks"
% One of the most prominent models is ARIMA [15]. Its statistical properties as well as the wellknown Box-Jenkins methodology [16] in the model selection procedure make it the first attempt for
%practitioners. However, its linear assumption and limited scalability make it unsuitable for large-scale
%forecasting tasks. Further, information across similar time series cannot be shared since each time
%series is fitted individually.

It is not hard to imagine that this wide usage brought time series a lot of attention in the past. One of the most widely known method for their forecasting is ARIMA (autoregressive integrated moving average). Despite its popularity, it comes with some disadvantages, like the necessity to fit it separately for every similar time series and for the user to have some expertise in the parameter selection procedure.

This brings us to the next era of the time series forecasting: deep neural networks.
The achievements in the natural language processing (NLP) field, including RNN and Transformer self-attention models have almost directly transferred to time series, due to a strikingly similar nature of both problems. However, RNNs and their successors (LSTM \cite{lstm}, GRU \cite{gru}) have difficulty with long sequences \cite{context}, causing them to struggle with time series that can contain long-term dependencies.


Transformer's self-attention \cite{tr} has revolutionized the world of long sequence processing, making it possible for the model to deal with larger-scale forecasting. However, there are several problems with integrating it directly for the time series forecasting usage: its local context-blindness \cite{enhancing} and large space-complexity growing quadratically with the sequence length.
%While self-attention is quite space-consuming (its complexity grows quadratically with sequence length [SOURCE, POSSIBLY FROM ENHANCING]), several solutions were proposed to make its usage feasible for capturing long-term dependencies.
One solution was proposed in \cite{enhancing} by introducing convolutional self-attention and LogSparse Transformer.




% TODO:  However, canonical dot-product
%self-attention matches queries against keys insensitive to local context,
% OGARNIJ O CO CHODZI.

%Time series forecasting is one of the most widespread type of data analysis, making it incredibly useful for various industries and research areas.
%It is
%
%Time series are one of the most widespread type of data, making  

\section{Problem statement}\label{r:problemst}

Transformer proposed in \cite{enhancing} is centered around the idea of every point in a time series being drawn from some Gaussian distribution. Thus, the model's goal is to predict the parameters of these distributions, ensuring that each prediction is partially random and introducing Gaussian noise to it.

In reality, it's not always the case that every time series' noise comes from a Gaussian distribution.
It would be more useful if we could model any continuous distribution instead. One way to approximate it is by using a discrete distribution with categories that are granular enough. Obviously, this comes with a disadvantage of being able to predict only a discretely limited value, which could cost some accuracy. However, the expectation is that this problem can be solved easily by just using a sufficiently large number of categories.

The goal of our project was using the Transformer model to predict time series assuming that each point in the sequences belongs to a categorical distribution.


%One interesting direction that was not thoroughly explored 

\section{Contributions}\label{r:contributions}

Work described in this thesis has been done via a collaboration with scientists from Institute of Mathematics of Polish Academy of Sciences.

My most important contributions are:
\begin{enumerate}
	\item Gaussian distribution-based Transformer
	\item Experiments with the Gaussian distribution-based Transformer
	\item Base implementation of the dataset feeder
	\item Time features as in \cite{enhancing} (on the data feeding side)
\end{enumerate}

\chapter{Background}\label{r:background}

\section{Notation}

For a time series represented as a vector $y$, $y^{(i)}_t$ denotes $t$ time step of a series number $i$.

\section{Time series}

%\begin{defi}\label{ts}
%%	\textbf{Time series} is an ordered sequence of datapoints, that were measured at uniform time intervals. We refer to the length of these intervals as time series' \textbf{frequency}.
%	We define a \textbf{time series} as a sequence of numbers measured across time with a constant \textbf{frequency}.
%	
%\end{defi}
	We define a \textbf{time series} as a sequence of numbers measured across time with a constant \textbf{frequency}.

\section{Causal convolution}
\begin{defi}\label{ts}
\textbf{Causal convolution} is a convolution used on input padded with $k - 1$ zeros at the start, where $k$ denotes the size of the kernel.
	
\end{defi}

This type of convolution is particularly useful for time series, since it enables us to make predictions with only partial input, essentially generating a completely new time series.


\section{Attention}

Attention is the key part to explaining the way Transformer works. It allows us to train an approximated dictionary which encodes the relations between different parts of the input.

One needs to provide three matrices to the attention layer: keys, queries and values. They are all created from the input in a way that depends on the attention function.


\subsection{Scaled Dot-Product Attention}

Scaled dot-product attention is perhaps the most known kind of attention function, which was introduced in \cite{tr}. Matrices $Q, K, V$ (respectively queries, keys and values) are created by multiplying the input matrix with three distinct weight matrices $W_Q, W_K, W_V$.

For given $Q, K, V$ we compute the attention with the following formula:

$$ Attention(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d_k}} \cdot M)V \textrm{ (from \cite{enhancing})} $$

where $d_k$ stands for the dimension of $Q$ and $K$. Dividing by this value is supposed to balance out the growth of $Q$ and $K^{T}$ dot product with larger values of $d_k$.

$M$ is a mask matrix with zeros in the upper triangle part and ones in the rest. It prevents the model from attempting to relate the past events to the future ones.


The dot-product attention does not use the information about input's general shape, which is a property of the classical Transformer that we call \textbf{context-blindness}. This means that points with similar values can be assigned a strong attention score, even if they come from dissimilar (differently-shaped) events.
% TODO: moze dopowiedz ze chodzi o softmaxa.

\subsection{Convolutional Attention}

The convolutional attention \cite{enhancing} is calculated in almost the same way as the scaled dot-product attention. 
It only differs in the way it computes queries and keys.
Instead of applying a matrix multiplication with weight matrices, it uses a causal convolution with a stride of 1. When the kernel's size is equal to 1 it is equal to the matrix multiplication.

The usage of convolution is supposed to offset the dot-product attention's context-blindness. Since the convolution layer learns to recognize input's shape, stronger attention score should be assigned to similar events.

\subsection{Multi-headed attention}

Each attention block might contain more than one parallel attention layer, aka attention "head". In theory this allows for different layers to learn other kind of relationships in the input and is a very successful tool e.g. in natural language processing \cite{tr}.

The outputs of all attention heads are concatenated before passing them down to the next block.


% jest causal

%\section{Covariates}
%
%Following \cite{enhancing}, we use additional variables as part of the model's input aside the time series itself.
%There are three kinds of covariates we use:
%\begin{enumerate}
%	\item Information about time the input was recorded (year, month, day-of-the-week, hour-of-the-day, minute-of-the-hour)
%	\item ID of the time series
%	\item Input's distance from the sequence's start
%\end{enumerate}
%These variables can contain the information about time the input was recorded (year, month, day-of-the-week, hour-of-the-day, minute-of-the-hour), ID of the time series or the input's distance from the sequence's start.


\section{Architecture}

\subsection{Overview}

The Transformer in \cite{enhancing} is a modification of the original Transformer \cite{tr}, that uses only a decoder.
We follow the same architecture type and therefore whenever we speak about the Transformer we will mean a decoder-only \cite{wikipedia} architecture one.

Our Transformer consists of a causal convolution layer, positional encoding layer, $N$ decoder blocks and a layer normalization.

\subsection{Positional encoding}

Positional encoding is a mapping of each position in the input to a vector.
A positional encoding layer sums the (embedded) input with its positional encoding matrix.

\subsection{Decoder}

The decoder block consists of a causal attention block and a feed-forward block. Both blocks are residual.

\section{Metrics}

\subsection{Quantile loss}

Quantile loss is a metric designed to evaluate and/or train models designed with quantile prediction in mind.
We define $\rho$-quantile loss as follows:

$$ QL_\rho(y, \hat{y}) = max(\rho(y - \hat{y}), (\rho - 1)(y - \hat{y})) \text{,} $$

where $y$ denotes the model's prediction for the $\rho$ quantile and $\hat{y}$ is the ground truth value.

This metric is characterized by a stronger penalization of over-predictions for $\rho$ smaller than 0.5, and under-predictions for $\rho$ greater than 0.5.

\subsection{Risk metric}

$Risk_\rho$ is a type of normalized quantile loss metric defined as follows for $\rho \in (0,1)$:

$$ Risk_\rho(\textbf{y}, \hat{\textbf{y}}) 
= 2\frac{\sum_{i,t}QL_\rho(y^{(i)}_t,\hat{y}^{(i)}_t)}
{\sum_{i,t} |y^{(i)}_t|}$$

%, where $y^{(i)}_t$ denotes $t$ time step of a series number $i$.

It is very popular in time series prediction papers that use the Transformer (e.g. in \cite{enhancing}, \cite{deepar}), which lead us to choose it as the metric we use in the model's evaluation.

\section{Training}

\subsection{Covariates}

Following \cite{enhancing}, we use additional variables as part of the model's input aside the time series itself.
There are three kinds of covariates we use:
\begin{enumerate}
	\item Numerical information about time the input was recorded (year, month, day-of-the-week, hour-of-the-day, minute-of-the-hour). We normalize them and store in one dimension.
	\item Embedded ID of the time series with the same dimensionality as positional embeddings.
	\item Input's distance from the sequence's start.
\end{enumerate}

The input to our model consists of summed ID embeddings and positional embeddings concatenated with rest of the covariates.

%\subsection{Scale handling}

\subsection{Weighted sampling}

We adapt a weighted sampling technique in our training, similarly to \cite{enhancing} and \cite{deepar}. Its main purpose is to counteract model's tendency to underfit data of a large scale, since it occurs more rarely in the datasets. We test that idea by including the weighted versus uniform sampling in our grid searches.

We calculate the weights by normalizing the scale factor $v_i$:

$$ v_i = 1 + \frac{\sum^{t_0}_{t=1} y^{(i)}_t}{t_0} $$


%\begin{defi}
%	$\rho$-quantile loss
%\end{defi}

%We define $\rho$-quantile loss for $\rho \in (0,1)$ as follows:
%
%$$ R_\rho(x, ) $$

%\begin{defi}
%	\textbf{The encoder} is a stack of identical layers. Each layer has two sub-layers.
%\end{defi}
%We use a standard "scaled dot-product attention"\cite{tr}.
%We can define self-attention as a function that takes as an input a query and 


% Causal convolution: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-padding-with-keras.md



\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliografia}

%\bibliography{bibFile}


%@article{einstein,
%	author =       "Albert Einstein",
%	title =        "On the electrodynamics of moving bodies",
%	journal =      "Annalen der Physik",
%	volume =       "322",
%	number =       "10",
%	pages =        "891--921",
%	year =         "1905"
%}
%
\bibitem{enhancing} Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang and Xifeng Yan. \textit{Enhancing the Locality and Breaking the Memory
	Bottleneck of Transformer on Time Series Forecasting}. arXiv:1907.00235, 2019.

\bibitem{lstm} Sepp Hochreiter and Jürgen Schmidhuber. \textit{Long short-term memory}. Neural computation, 9(8):1735–1780, 1997.

\bibitem{gru} Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. \textit{On the properties of
	neural machine translation: Encoder-decoder approaches}. arXiv preprint arXiv:1409.1259, 2014.

\bibitem{context} Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. \textit{Sharp nearby, fuzzy far away: How neural language models use context}. arXiv preprint arXiv:1805.04623, 2018.

\bibitem{tr} Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. \textit{Attention is all you need}. NIPS 2017.

\bibitem{wikipedia} Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. \textit{Generating wikipedia by summarizing long sequences}. arXiv preprint arXiv:1801.10198, 2018.

\bibitem{deepar} Valentin Flunkert, David Salinas, and Jan Gasthaus. \textit{Deepar: Probabilistic forecasting with autoregressive recurrent networks}. arXiv preprint arXiv:1704.04110, 2017.

%\bibitem[Blar16]{eb1} Elizjusz Blarbarucki, \textit{O pewnych
%    aspektach pewnych aspektów}, Astrolog Polski, Zeszyt 16, Warszawa
%  1916.
%
%\bibitem[Fif00]{ffgg} Filigran Fifak, Gizbert Gryzogrzechotalski,
%  \textit{O blabalii fetorycznej}, Materiały Konferencji Euroblabal
%  2000.
%
%\bibitem[Fif01]{ff-sr} Filigran Fifak, \textit{O fetorach
%    $\sigma$-$\rho$}, Acta Fetorica, 2001.
%
%\bibitem[Głomb04]{grglo} Gryzybór Głombaski, \textit{Parazytonikacja
%    blabiczna fetorów --- nowa teoria wszystkiego}, Warszawa 1904.
%
%\bibitem[Hopp96]{hopp} Claude Hopper, \textit{On some $\Pi$-hedral
%    surfaces in quasi-quasi space}, Omnius University Press, 1996.
%
%\bibitem[Leuk00]{leuk} Lechoslav Leukocyt, \textit{Oval mappings ab ovo},
%  Materiały Białostockiej Konferencji Hodowców Drobiu, 2000.
%
%\bibitem[Rozk93]{JR} Josip A.~Rozkosza, \textit{O pewnych własnościach
%    pewnych funkcji}, Północnopomorski Dziennik Matematyczny 63491
%  (1993).
%
%\bibitem[Spy59]{spyrpt} Mrowclaw Spyrpt, \textit{A matrix is a matrix
%    is a matrix}, Mat. Zburp., 91 (1959) 28--35.
%
%\bibitem[Sri64]{srinis} Rajagopalachari Sriniswamiramanathan,
%  \textit{Some expansions on the Flausgloten Theorem on locally
%    congested lutches}, J. Math.  Soc., North Bombay, 13 (1964) 72--6.
%
%\bibitem[Whi25]{russell} Alfred N. Whitehead, Bertrand Russell,
%  \textit{Principia Mathematica}, Cambridge University Press, 1925.
%
%\bibitem[Zen69]{heu} Zenon Zenon, \textit{Użyteczne heurystyki
%    w~blabalizie}, Młody Technik, nr~11, 1969.

\end{thebibliography}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% coding: latin-2
%%% End:
