\BOOKMARK [0][-]{chapter*.2}{Introduction}{}% 1
\BOOKMARK [0][-]{chapter.1}{Background}{}% 2
\BOOKMARK [1][-]{section.1.1}{Time series and notation}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.2}{Transformer model architecture}{chapter.1}% 4
\BOOKMARK [2][-]{subsection.1.2.1}{Overview}{section.1.2}% 5
\BOOKMARK [1][-]{section.1.3}{Attention}{chapter.1}% 6
\BOOKMARK [2][-]{subsection.1.3.1}{Scaled Dot-Product Attention}{section.1.3}% 7
\BOOKMARK [2][-]{subsection.1.3.2}{Convolutional Attention}{section.1.3}% 8
\BOOKMARK [2][-]{subsection.1.3.3}{Multi-headed attention}{section.1.3}% 9
\BOOKMARK [1][-]{section.1.4}{Training}{chapter.1}% 10
\BOOKMARK [2][-]{subsection.1.4.1}{Additional input}{section.1.4}% 11
\BOOKMARK [2][-]{subsection.1.4.2}{Scaling and weighted sampling}{section.1.4}% 12
\BOOKMARK [1][-]{section.1.5}{Metrics}{chapter.1}% 13
\BOOKMARK [2][-]{subsection.1.5.1}{Quantile loss}{section.1.5}% 14
\BOOKMARK [2][-]{subsection.1.5.2}{Risk metric}{section.1.5}% 15
\BOOKMARK [0][-]{chapter.2}{Approximating a continuous distribution with the Transformer}{}% 16
\BOOKMARK [1][-]{section.2.1}{Serial Predictor}{chapter.2}% 17
\BOOKMARK [2][-]{subsection.2.1.1}{Discretization}{section.2.1}% 18
\BOOKMARK [2][-]{subsection.2.1.2}{Architecture's summary}{section.2.1}% 19
\BOOKMARK [0][-]{chapter.3}{Experiments}{}% 20
\BOOKMARK [1][-]{section.3.1}{Datasets}{chapter.3}% 21
\BOOKMARK [1][-]{section.3.2}{Differences from previous works}{chapter.3}% 22
\BOOKMARK [1][-]{section.3.3}{Experimental setup}{chapter.3}% 23
\BOOKMARK [1][-]{section.3.4}{Results}{chapter.3}% 24
\BOOKMARK [0][-]{chapter.4}{Conclusion}{}% 25
\BOOKMARK [1][-]{section.4.1}{Summary}{chapter.4}% 26
\BOOKMARK [1][-]{section.4.2}{Further work}{chapter.4}% 27
\BOOKMARK [0][-]{chapter*.8}{Bibliography}{}% 28
